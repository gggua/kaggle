{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import copy\n",
    "pd.set_option('max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_res = pd.read_csv(r'G:\\machine learning\\kaggle\\NACCM-2020\\MDataFiles_Stage1\\MNCAATourneyDetailedResults.csv')\n",
    "season_res = pd.read_csv(r'G:\\machine learning\\kaggle\\NACCM-2020\\MDataFiles_Stage1\\MRegularSeasonDetailedResults.csv')\n",
    "seeds = pd.read_csv(r'G:\\machine learning\\kaggle\\NACCM-2020\\MDataFiles_Stage1\\MNCAATourneySeeds.csv')\n",
    "sample_sub = pd.read_csv(r'G:\\machine learning\\kaggle\\NACCM-2020\\MSampleSubmissionStage1_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_data(df):\n",
    "    df['WOR'] = df['WOR'] + df['WDR']\n",
    "    df['LOR'] = df['LOR'] + df['LDR']\n",
    "    df = df.rename(columns={'WOR':'WR','LOR':'LR'})\n",
    "    df = df.drop(['WFGM3','WFGA3','WFTM','WFTA','WDR','WTO','WPF','LFGM3','LFGA3','LFTM','LFTA','LDR','LTO','LPF'],axis=1)\n",
    "    return df\n",
    "tourney_res = fix_data(tourney_res)\n",
    "season_res = fix_data(season_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['WScore','LScore','WFGM','WFGA','WR','WAst','WStl','WBlk','LFGM','LFGA','LR','LAst','LStl','LBlk']\n",
    "for column in columns:\n",
    "    season_res.loc[season_res['NumOT']!=0, column] = season_res.loc[season_res['NumOT']!=0, column]*(40/(40+season_res['NumOT']*5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_data(df):\n",
    "    df_swap = df[['Season','DayNum','LTeamID','LScore','WTeamID','WScore','WLoc','NumOT','LFGM','LFGA','LR','LAst','LStl','LBlk','WFGM','WFGA','WR','WAst','WStl','WBlk']]\n",
    "    df_swap.loc[df['WLoc']=='A','WLoc']='H'\n",
    "    df_swap.loc[df['WLoc']=='H','WLoc']='A'\n",
    "    df = df.rename(columns={'WLoc':'location'})\n",
    "    df_swap = df_swap.rename(columns={'WLoc':'location'})\n",
    "    df.columns=[x.replace('W','T1_').replace('L','T2_') for x in list(df.columns)]\n",
    "    df_swap.columns=[x.replace('L','T1_').replace('W','T2_') for x in list(df_swap.columns)]\n",
    "    res = pd.concat([df,df_swap]).sort_index(axis=0).reset_index(drop=True)\n",
    "    return res\n",
    "tourney_res = double_data(tourney_res)\n",
    "season_res = double_data(season_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_res['T1_TeamID'] = season_res['T1_TeamID'].astype(str)\n",
    "season_res['T2_TeamID'] = season_res['T2_TeamID'].astype(str)\n",
    "\n",
    "season_res['win'] = np.where(season_res['T1_Score']>season_res['T2_Score'], 1, 0)\n",
    "\n",
    "def team_quality(season):\n",
    "    formula = 'win~-1+T1_TeamID+T2_TeamID'\n",
    "    glm = sm.GLM.from_formula(formula=formula, \n",
    "                              data=season_res.loc[season_res.Season==season,:], \n",
    "                              family=sm.families.Binomial()).fit()\n",
    "    quality = pd.DataFrame(glm.params).reset_index()\n",
    "    quality.columns = ['TeamID','beta']\n",
    "    quality['Season'] = season\n",
    "    quality['quality'] = np.exp(quality['beta'])\n",
    "    quality = quality.loc[quality.TeamID.str.contains('T1_')].reset_index(drop=True)\n",
    "    quality['TeamID'] = quality['TeamID'].apply(lambda x: x[10:14]).astype(int)\n",
    "    return quality\n",
    "\n",
    "\n",
    "team_quality = pd.concat([team_quality(2003),team_quality(2004),team_quality(2005),team_quality(2006),team_quality(2007),\n",
    "                          team_quality(2008),team_quality(2009),team_quality(2010),team_quality(2011),team_quality(2012),\n",
    "                          team_quality(2013),team_quality(2014),team_quality(2015),team_quality(2016),team_quality(2017),\n",
    "                          team_quality(2018),team_quality(2019)]).reset_index(drop=True)\n",
    "season_res['T1_TeamID'] = season_res['T1_TeamID'].astype(int)\n",
    "season_res['T2_TeamID'] = season_res['T2_TeamID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_quality_T1 = team_quality[['TeamID','Season','quality']]\n",
    "team_quality_T1.columns = ['T1_TeamID','Season','T1_quality']\n",
    "team_quality_T2 = team_quality[['TeamID','Season','quality']]\n",
    "team_quality_T2.columns = ['T2_TeamID','Season','T2_quality']\n",
    "\n",
    "tourney_res['T1_TeamID'] = tourney_res['T1_TeamID'].astype(int)\n",
    "tourney_res['T2_TeamID'] = tourney_res['T2_TeamID'].astype(int)\n",
    "tourney_res = pd.merge(tourney_res,team_quality_T1, on = ['T1_TeamID','Season'], how = 'left')\n",
    "tourney_res = pd.merge(tourney_res,team_quality_T2, on = ['T2_TeamID','Season'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds['seed'] = seeds['Seed'].apply(lambda x: int(x[1:3]))\n",
    "seeds['division'] = seeds['Seed'].apply(lambda x: x[0])\n",
    "\n",
    "seeds_T1 = seeds[['Season','TeamID','seed','division']].copy()\n",
    "seeds_T2 = seeds[['Season','TeamID','seed','division']].copy()\n",
    "seeds_T1.columns = ['Season','T1_TeamID','T1_seed','T1_division']\n",
    "seeds_T2.columns = ['Season','T2_TeamID','T2_seed','T2_division']\n",
    "\n",
    "tourney_res = pd.merge(tourney_res,seeds_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "tourney_res = pd.merge(tourney_res,seeds_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "tourney_res['diff_seed'] = tourney_res['T1_seed'] - tourney_res['T2_seed']\n",
    "tourney_res = tourney_res.drop(['T1_seed','T2_seed'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_res['T1_powerrank'] = tourney_res.groupby(['Season','T1_division'])['T1_quality'].rank(method='dense', ascending=False).astype(int)\n",
    "tourney_res['T2_powerrank'] = tourney_res.groupby(['Season','T2_division'])['T2_quality'].rank(method='dense', ascending=False).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_res['win'] = np.where((season_res['DayNum']>98)&(season_res['T1_Score']>season_res['T2_Score']), 1, 0)\n",
    "season_res['games'] = np.where(season_res['DayNum']>98, 1, 0)\n",
    "T1_season_summary = season_res.groupby(['Season','T1_TeamID']).agg({'win':'sum'}).reset_index()\n",
    "\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'games':'sum'})\n",
    "group.columns = ['play_games']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'],how='left')\n",
    "\n",
    "T1_season_summary['T1_win30days_ratio'] = T1_season_summary['win']/T1_season_summary['play_games']\n",
    "season_res = season_res.drop(['win','games'],axis=1)\n",
    "T1_season_summary = T1_season_summary.drop(['win','play_games'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T1_Score':'mean'})\n",
    "group.columns = ['T1_mean_Score']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')\n",
    "\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T1_Score':'median'})\n",
    "group.columns = ['T1_median_Score']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')\n",
    "\n",
    "season_res['T1_Diff_Score'] = season_res['T1_Score'] - season_res['T2_Score']\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T1_Diff_Score':'mean'})\n",
    "group.columns = ['T1_mean_diff_score']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season', 'T1_TeamID'], how='left')\n",
    "season_res = season_res.drop(['T1_Diff_Score'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_res['T1_PFG'] = round(season_res['T1_FGM']/season_res['T1_FGA'],4)\n",
    "season_res['T2_PFG'] = round(season_res['T2_FGM']/season_res['T2_FGA'],4)\n",
    "\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T1_PFG':'min'})\n",
    "group.columns = ['T1_min_PFG']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')\n",
    "\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T1_PFG':'median'})\n",
    "group.columns = ['T1_median_PFG']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')\n",
    "\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T1_PFG':'max'})\n",
    "group.columns = ['T1_max_PFG']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')\n",
    "\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T2_PFG':'min'})\n",
    "group.columns = ['T1_opposite_min_PFG']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T1_FGM':'sum'})\n",
    "group.columns = ['T1_season_FGM']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T1_FGA':'sum'})\n",
    "group.columns = ['T1_season_FGA']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')\n",
    "T1_season_summary['T1_season_PFG'] = round(T1_season_summary['T1_season_FGM']/T1_season_summary['T1_season_FGA'], 4)\n",
    "T1_season_summary = T1_season_summary.drop(['T1_season_FGM', 'T1_season_FGA'], axis=1)\n",
    "\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T2_FGM':'sum'})\n",
    "group.columns = ['T2_season_FGM']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T2_FGA':'sum'})\n",
    "group.columns = ['T2_season_FGA']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')\n",
    "T1_season_summary['T1_opposite_season_PFG'] = round(T1_season_summary['T2_season_FGM']/T1_season_summary['T2_season_FGA'], 4)\n",
    "T1_season_summary = T1_season_summary.drop(['T2_season_FGM', 'T2_season_FGA'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T1_R':'mean'})\n",
    "group.columns = ['T1_season_R']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')\n",
    "\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T1_Ast':'mean'})\n",
    "group.columns = ['T1_season_Ast']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')\n",
    "\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T1_Stl':'mean'})\n",
    "group.columns = ['T1_season_Stl']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')\n",
    "\n",
    "group = season_res.groupby(['Season','T1_TeamID']).agg({'T1_Blk':'mean'})\n",
    "group.columns = ['T1_season_Blk']\n",
    "T1_season_summary = pd.merge(T1_season_summary, group, on=['Season','T1_TeamID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2_season_summary = copy.deepcopy(T1_season_summary)\n",
    "T2_season_summary.columns = [x.replace('T1_','T2_') for x in list(T2_season_summary.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_res['result'] = np.where(tourney_res['T1_Score']>tourney_res['T2_Score'], 1, 0)\n",
    "tourney_res = tourney_res.drop(['DayNum','T1_Score','T2_Score','NumOT','T1_FGM','T1_FGA','T1_R','T1_Ast','T1_Stl',\n",
    "                             'T1_Blk','T2_FGM','T2_FGA','T2_R','T2_Ast','T2_Stl','T2_Blk'],axis=1)\n",
    "\n",
    "tourney_res = pd.merge(tourney_res, T1_season_summary, on=['Season','T1_TeamID'], how='left')\n",
    "tourney_res = pd.merge(tourney_res, T2_season_summary, on=['Season','T2_TeamID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sub['Season'] = sample_sub['ID'].apply(lambda x:int(x[0:4]))\n",
    "sample_sub['T1_TeamID'] = sample_sub['ID'].apply(lambda x:int(x[5:9]))\n",
    "sample_sub['T2_TeamID'] = sample_sub['ID'].apply(lambda x:int(x[-4:]))\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['ID'] = sample_sub['ID']\n",
    "sample_sub = sample_sub.drop(['ID','Pred'], axis=1)\n",
    "\n",
    "# sample_sub1 = copy.deepcopy(sample_sub)\n",
    "# sample_sub2 = copy.deepcopy(sample_sub)\n",
    "# sample_sub3 = copy.deepcopy(sample_sub)\n",
    "\n",
    "sample_sub = pd.merge(sample_sub,team_quality_T1, on = ['T1_TeamID','Season'], how = 'left')\n",
    "sample_sub = pd.merge(sample_sub,team_quality_T2, on = ['T2_TeamID','Season'], how = 'left')\n",
    "\n",
    "sample_sub = pd.merge(sample_sub,seeds_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "sample_sub = pd.merge(sample_sub,seeds_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "sample_sub['diff_seed'] = sample_sub['T1_seed'] - sample_sub['T2_seed']\n",
    "sample_sub = sample_sub.drop(['T1_seed','T2_seed'], axis=1)\n",
    "\n",
    "sample_sub['T1_powerrank'] = tourney_res.groupby(['Season','T1_division'])['T1_quality'].rank(method='dense', ascending=False).astype(int)\n",
    "sample_sub['T2_powerrank'] = tourney_res.groupby(['Season','T2_division'])['T2_quality'].rank(method='dense', ascending=False).astype(int)\n",
    "\n",
    "sample_sub = pd.merge(sample_sub, T1_season_summary, on=['Season','T1_TeamID'], how='left')\n",
    "sample_sub = pd.merge(sample_sub, T2_season_summary, on=['Season','T2_TeamID'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = tourney_res['result']\n",
    "tourney_res = tourney_res.drop(['result','location'], axis=1)\n",
    "train_x = tourney_res\n",
    "test_data = sample_sub\n",
    "train_x = pd.get_dummies(train_x)\n",
    "test_data = pd.get_dummies(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score,train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm.sklearn import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\engine.py:113: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:1184: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "G:\\anaconda\\lib\\site-packages\\lightgbm\\basic.py:742: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    }
   ],
   "source": [
    "train_x = pd.get_dummies(train_x)\n",
    "test_data = pd.get_dummies(test_data)\n",
    "\n",
    "sub['pred'] = 0\n",
    "temp = np.zeros(test_data.shape[0])\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle = True, random_state= 12)\n",
    "params = {'n_jobs':-1, 'learning_rate':0.01, 'n_estimators':1300, 'max_depth':8,\n",
    "          'num_leaves':31, 'reg_alpha':1, 'reg_lambda':1, 'min_child_samples':20,\n",
    "          'min_split_gain':0.7, 'colsample_bytree':0.3}\n",
    "for train, test in kfold.split(train_x):\n",
    "    X_train = train_x.iloc[train]\n",
    "    y_train = train_y.iloc[train]\n",
    "    X_test = train_x.iloc[test]\n",
    "    y_test = train_y.iloc[test]\n",
    "    lgb_train = lgb.Dataset(X_train, label=y_train, categorical_feature=['T1_division_W','T1_division_X','T1_division_Y','T1_division_Z',\n",
    "                                                                         'T2_division_W','T2_division_X','T2_division_Y','T2_division_Z'])\n",
    "    lgb_eval = lgb.Dataset(X_test, label=y_test, categorical_feature=['T1_division_W','T1_division_X','T1_division_Y','T1_division_Z',\n",
    "                                                                         'T2_division_W','T2_division_X','T2_division_Y','T2_division_Z'])\n",
    "    gbm = lgb.train(params, lgb_train, num_boost_round=500, valid_sets=lgb_eval)\n",
    "    y_pred = gbm.predict(test_data)\n",
    "    temp += y_pred/10\n",
    "sub['pred'] = temp\n",
    "sub.to_csv('lgb2_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1811592398939186\n",
      "0.18691280303307936\n",
      "0.1902236714284837\n",
      "0.17197311671038196\n",
      "0.18201892986907375\n",
      "0.19328035409204\n",
      "0.20176587229602908\n",
      "0.19924814851806535\n",
      "0.20510735399705904\n",
      "0.18401838384085295\n"
     ]
    }
   ],
   "source": [
    "train_x = pd.get_dummies(train_x)\n",
    "test_data = pd.get_dummies(test_data)\n",
    "\n",
    "sub['pred'] = 0\n",
    "temp = np.zeros(test_data.shape[0])\n",
    "kfold = KFold(n_splits=10, shuffle = True, random_state= 12)\n",
    "lgb_model = LGBMRegressor(n_jobs=-1,\n",
    "                          learning_rate=0.01,\n",
    "                          n_estimators=1300,\n",
    "                          max_depth=8,\n",
    "                          num_leaves=31,\n",
    "                          reg_alpha=1, \n",
    "                          reg_lambda=1,\n",
    "                          min_child_samples=20,\n",
    "                          min_split_gain=0.7,\n",
    "                          colsample_bytree=0.3)\n",
    "for train, test in kfold.split(train_x):\n",
    "    X_train = train_x.iloc[train]\n",
    "    y_train = train_y.iloc[train]\n",
    "    X_test = train_x.iloc[test]\n",
    "    y_test = train_y.iloc[test]\n",
    "    lgb_model.fit(X_train,y_train)\n",
    "    y_pred = lgb_model.predict(X=X_test)\n",
    "    e = mean_squared_error(y_true=y_test,y_pred=y_pred)\n",
    "    print(e)\n",
    "lgb_pred = lgb_model.predict(test_data)\n",
    "sub['pred'] = lgb_pred\n",
    "sub.to_csv('lgb_submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 27 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 108 out of 108 | elapsed: 25.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.9, 'learning_rate': 0.0005, 'max_depth': 10}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "cv_params = {'colsample_bytree':[0.7,0.8,0.9],\n",
    "             'max_depth':[10,20,30],\n",
    "             'learning_rate':[0.0001,0.0005,0.001]}\n",
    "xgb_cv_model = xgb.XGBRegressor(colsample_bytree=0.9,\n",
    "                          learning_rate=0.0005,\n",
    "                          max_depth=10,\n",
    "                          subsample=0.5,\n",
    "                          objective='binary:logistic',\n",
    "                          eval_metric='logloss',\n",
    "                          min_child_weight=20,\n",
    "                          gamma=0.25,\n",
    "                          n_estimators=5000,\n",
    "                          verbosity=0)\n",
    "gs = GridSearchCV(xgb_cv_model,cv_params,scoring='r2',cv=4,verbose=1)\n",
    "gs_result = gs.fit(X_train,y_train)\n",
    "gs_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19744229108336459\n",
      "0.216075413049697\n",
      "0.19536075681035325\n",
      "0.2453998039855302\n",
      "0.23553662515278298\n",
      "0.2337900119355137\n",
      "0.20195461370685694\n",
      "0.1835823207066073\n",
      "0.20741240072745667\n",
      "0.20795498308960844\n"
     ]
    }
   ],
   "source": [
    "sub['pred'] = 0\n",
    "kfold = KFold(n_splits=10, shuffle = True, random_state= 42)\n",
    "xgb_model = xgb.XGBRegressor(colsample_bytree=0.98,\n",
    "                             learning_rate=0.005,\n",
    "                             max_depth=31,\n",
    "                             subsample=1,\n",
    "                             objective='binary:logistic',\n",
    "                             eval_metric='logloss',\n",
    "                             min_child_weight=3,\n",
    "                             gamma=0.25,\n",
    "                             n_estimators=5000,\n",
    "                             verbosity=0)\n",
    "for train, test in kfold.split(train_x):\n",
    "    X_train = train_x.iloc[train]\n",
    "    y_train = train_y.iloc[train]\n",
    "    X_test = train_x.iloc[test]\n",
    "    y_test = train_y.iloc[test]\n",
    "    xgb_model.fit(X_train,y_train)\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    e = mean_squared_error(y_true=y_test,y_pred=y_pred)\n",
    "    print(e)\n",
    "sub['pred'] = xgb_model.predict(test_data)\n",
    "sub.to_csv('xgb_submission5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub['pred'] = 0\n",
    "# temp = np.zeros(test_data.shape[0])\n",
    "\n",
    "# kfold = KFold(n_splits=10, shuffle = True, random_state= 42)\n",
    "# params = {'colsample_bytree':0.8,'learning_rate':0.0003,'max_depth':31,'subsample':1,'objective':'binary:logistic','eval_metric':'logloss',\n",
    "#           'min_child_weight':3,'gamma':0.25,'n_estimators':5000,'verbosity':0}\n",
    "# for train, test in kfold.split(train_x):\n",
    "#     X_train = train_x.iloc[train]\n",
    "#     y_train = train_y.iloc[train]\n",
    "#     X_test = train_x.iloc[test]\n",
    "#     y_test = train_y.iloc[test]\n",
    "#     lgb_train = lgb.Dataset(X_train, label=y_train, categorical_feature=['T1_division_W','T1_division_X','T1_division_Y','T1_division_Z',\n",
    "#                                                                          'T2_division_W','T2_division_X','T2_division_Y','T2_division_Z'])\n",
    "#     lgb_eval = lgb.Dataset(X_test, label=y_test, categorical_feature=['T1_division_W','T1_division_X','T1_division_Y','T1_division_Z',\n",
    "#                                                                          'T2_division_W','T2_division_X','T2_division_Y','T2_division_Z'])\n",
    "#     gbm = lgb.train(params, lgb_train, num_boost_round=500, valid_sets=lgb_eval)\n",
    "#     y_pred = gbm.predict(test_data)\n",
    "#     temp += y_pred/10\n",
    "# sub['pred'] = temp\n",
    "# sub.to_csv('lgb2_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 27 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 108 out of 108 | elapsed:   32.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.3, 'min_child_sample': 20, 'min_split_gain': 0.7}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# cv_params = {'min_child_sample':[20,30,40],\n",
    "#             'min_split_gain':[0.3,0.5,0.7],\n",
    "#             'colsample_bytree':[0.1,0.2,0.3]}\n",
    "# lgb_cv_model = LGBMRegressor(n_jobs=-1,\n",
    "#                           learning_rate=0.01,\n",
    "#                           n_estimators=1300,\n",
    "#                           max_depth=8,\n",
    "#                           num_leaves=31,\n",
    "#                           reg_alpha=1, \n",
    "#                           reg_lambda=1,\n",
    "#                           min_child_samples=20,\n",
    "#                           min_split_gain=0.7,\n",
    "#                           colsample_bytree=0.3)\n",
    "# gs = GridSearchCV(lgb_cv_model,cv_params,scoring='r2',cv=4,verbose=1)\n",
    "# gs_result = gs.fit(X_train,y_train)\n",
    "# gs_result.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
